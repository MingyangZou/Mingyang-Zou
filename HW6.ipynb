{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80ea39e",
   "metadata": {},
   "source": [
    "# Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6bef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_path = '/home/jovyan/STA365/tsla_2014_2023.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "df['price_increase_next_day'] = (df['next_day_close'] > df['close']).astype(int)\n",
    "df.drop(columns='next_day_close', inplace=True) \n",
    "\n",
    "X = df.drop(columns=['date', 'price_increase_next_day'])\n",
    "y = df['price_increase_next_day']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "n, p = X_scaled.shape\n",
    "\n",
    "with pm.Model() as logistic_model:\n",
    "    betas = pm.Normal('betas', mu=0, sigma=10, shape=p) \n",
    "    intercept = pm.Normal('intercept', mu=0, sigma=10) \n",
    "\n",
    "    linear_component = pm.math.dot(X_scaled, betas) + intercept\n",
    "    p_outcome = pm.math.sigmoid(linear_component)  \n",
    "    y_obs = pm.Bernoulli('y_obs', p=p_outcome, observed=y.values)\n",
    "    \n",
    "    trace = pm.sample(1000, target_accept=0.95, return_inferencedata=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba677c53",
   "metadata": {},
   "source": [
    "# Part 2.\n",
    "In Bayesian statistics, we express our beliefs about parameters using prior distributions. Incorporating these priors into our model, the posterior distribution reflects a regularization analogous to Lasso and Ridge.\n",
    "\n",
    "For a parameter vector $\\beta$:\n",
    "\n",
    "A Normal prior corresponds to L2 regularization (Ridge): $\\beta_j \\sim \\mathcal{N}(0, \\tau^2)$ where $\\tau^2 = 1/\\lambda$. The log posterior distribution (omitting constant terms for simplicity) with $\\sigma=1$ is proportional to: $-\\frac{1}{2}(y - X\\beta)^T(y - X\\beta) - \\lambda ||\\beta||_2^2$. This resembles the Ridge regression loss function, where the penalty term is the squared L2 norm of $\\beta$.\n",
    "\n",
    "A Laplace prior corresponds to L1 regularization (Lasso):$\\beta_j \\sim \\text{Laplace}(0, b)$ with $b = 1/\\sqrt{\\lambda}$. The log posterior, again omitting constants and with $\\sigma=1$, is proportional to: $-\\frac{1}{2}(y - X\\beta)^T(y - X\\beta) - \\lambda ||\\beta||_1$. This mirrors the Lasso regression loss function, where the penalty term is the L1 norm of $\\beta$.\n",
    "\n",
    "Bayesian inference updates parameter beliefs by combining prior information with data, yielding a posterior distribution. Unlike optimizing a single estimate, Bayesians sample from this distribution to understand parameter uncertainty. The prior effectively regularizes the estimation, guiding it towards plausible values, akin to how frequentist regularization penalizes model complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e516dd",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44531191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "with pm.Model() as robust_model:\n",
    "    beta = pm.Normal('beta', mu=0, sd=10, shape=X.shape[1])\n",
    "    intercept = pm.Normal('intercept', mu=0, sd=10)\n",
    "    w = pm.HalfNormal('w', sd=10)\n",
    "\n",
    "    nu = pm.Exponential('nu', 1/29) + 1\n",
    "\n",
    "    y_pred = pm.StudentT('y_pred', mu=intercept + pm.math.dot(X, beta),\n",
    "                         sd=w, nu=nu, observed=y)\n",
    "    \n",
    "\n",
    "    trace = pm.sample(1000, return_inferencedata=True)\n",
    "\n",
    "with robust_model:\n",
    "    ppc = pm.sample_posterior_predictive(trace, var_names=['y_pred'])\n",
    "\n",
    "\n",
    "residuals = y.values - ppc['y_pred'].mean(axis=0)\n",
    "std_residuals = residuals / residuals.std()\n",
    "\n",
    "outliers = np.where(np.abs(std_residuals) > 2)[0]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8291198e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
